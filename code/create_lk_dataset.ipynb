{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Young Host Stars Lightcurve Downloader\n",
    "\n",
    "In this notebook, we will use the `identify_young_stars.ipynb` file to identify young host stars and save the results as a database in `results/young_stars_below199Myr.csv`. Our goal is to determine how many of these stars have lightcurves available for download using the `lightkurve` Python library.\n",
    "\n",
    "To achieve this, we will create a new Jupyter notebook and import the necessary modules. We will then load the `young_stars_below199Myr.csv` file and loop through each star to search for its lightcurve using `lightkurve`. If a lightcurve is found, we will download it and save it to a folder for further analysis.\n",
    "\n",
    "By the end of this notebook, we will have a dataset of lightcurves for young host stars that can be used for various scientific analyses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightkurve as lk\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data from a CSV File\n",
    "\n",
    "We read data with information about the young host stars of interest from the `results/young_stars_below199Myr.csv` we generated with `identify_young_stars.ipynb` using the Nasa Exoplanet Database.\n",
    "\n",
    "We extract different star names for the same host star from our database, and by looping them we try to find a match for the name that `lightkurve` uses for the star. \n",
    "\n",
    "We generate a database with relevant information for each host star like:\n",
    "- If it has lightcurves available\n",
    "- How many of them \n",
    "- Number of lightcurves per cadence\n",
    "- Missions that retrieved lighcurves\n",
    "\n",
    "Database is saved in `results/TPF_dataframe.csv`.\n",
    "\n",
    "This can take up to half an hour to retrieve all the data for each host star, as we are looping through 247 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../results/young_stars_below_100Myr.csv'\n",
    "df = pd.read_csv(path_data, index_col=0)\n",
    "name_heads = ['hostname', 'hd_name', 'hip_name', 'tic_id', 'gaia_id']\n",
    "star_names = [list(df[name_head]) for name_head in name_heads]\n",
    "\n",
    "# Initialize DataFrame\n",
    "result_df = pd.DataFrame(columns=['Star_Name', 'TPF_Found', 'Found_Star_Name', 'Num_Results', 'Target_Name'])\n",
    "\n",
    "for j in range(len(star_names[0])):\n",
    "\n",
    "    # Initialize row dictionary\n",
    "    row_data = {}\n",
    "    \n",
    "    row_data['Star_Name'] = star_names[0][j]\n",
    "    \n",
    "    for i in range(len(star_names)):\n",
    "        star_name = star_names[i][j]\n",
    "        \n",
    "        if star_name is not np.nan:\n",
    "            tpf = lk.search_targetpixelfile(star_name)\n",
    "\n",
    "            \n",
    "            if len(tpf) > 0:\n",
    "                missions = list(set(list(tpf.table['obs_collection'])))\n",
    "                missions_str = '/'.join(missions)\n",
    "                row_data['TPF_Found'] = True\n",
    "                row_data['Mission'] = missions_str\n",
    "                row_data['Found_Star_Name'] = star_name\n",
    "                row_data['Num_Results'] = len(tpf)\n",
    "                row_data['Target_Name'] = tpf.table['target_name'][0]\n",
    "                \n",
    "                # Adding dynamic columns for exptime\n",
    "                for exptime in tpf.table['exptime']:\n",
    "                    col_name = f'exptime_{exptime:.0f}'\n",
    "                    if col_name not in result_df.columns:\n",
    "                        result_df[col_name] = 0\n",
    "                    row_data[col_name] = row_data.get(col_name, 0) + 1\n",
    "                \n",
    "                break  # Exit if TPF is found\n",
    "            else:\n",
    "                row_data['TPF_Found'] = False\n",
    "                \n",
    "    # Append the row to DataFrame\n",
    "    result_df = pd.concat([result_df, pd.DataFrame(row_data, index=[0])], ignore_index=True)\n",
    "    # make the column 'Mission' the third one\n",
    "\n",
    "\n",
    "# Replace NaNs with appropriate defaults (e.g., 0 or False)\n",
    "result_df.fillna({'TPF_Found': False, 'Found_Star_Name': 'Not Found', 'Num_Results': 0}, inplace=True)\n",
    "result_df.fillna(0, inplace=True)\n",
    "\n",
    "savefold = '../results/'\n",
    "if not os.path.exists(savefold):\n",
    "    # create the folder if it does not exist\n",
    "    os.makedirs(savefold)\n",
    "figname = f'TPF_dataframe.csv'\n",
    "savepath = savefold + figname\n",
    "result_df.to_csv(savepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the data\n",
    "\n",
    "We use the dataframe we just generated to download the data. This cell can be run without running the previous on eif the dataset has already been generated. For that we retrieve the all the Target Pixel Files  (TPFs) for each host star. We use the in-built pipeline to mask the data and integrate the lightcurve.   \n",
    "\n",
    "Lightcurves are saved in `results/TPF_data` as `.fits` files and classified them by cadence by subfolder. \n",
    "\n",
    "The downloading can take up to an hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 light curves found out of 247 stars.\n"
     ]
    }
   ],
   "source": [
    "path_data = '../results/TPF_dataframe.csv'\n",
    "df = pd.read_csv(path_data)\n",
    "\n",
    "n_light = len(df[df['TPF_Found'] == True])\n",
    "\n",
    "print(f'{n_light} light curves found out of {len(df)} stars.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder = '../results/TPF_data/'\n",
    "subfolders = [f.path for f in os.scandir(folder) if f.is_dir()]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # Check if TPF was found for this star\n",
    "    if row['TPF_Found']:\n",
    "        # Create a folder for the star\n",
    "        name = row['Star_Name']\n",
    "        \n",
    "        #join name with underscores\n",
    "        name = name.replace(\" \", \"_\")\n",
    "        star_folder = f\"{folder}{name}\"\n",
    "\n",
    "        if star_folder in subfolders:\n",
    "            print(f\"Folder for {name} already exists. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        os.makedirs(star_folder, exist_ok=True)\n",
    "        # Search for the TPF\n",
    "        found_name = row['Found_Star_Name']\n",
    "        tpf = lk.search_targetpixelfile(row['Found_Star_Name'])\n",
    "        n_tpf = len(tpf)\n",
    "\n",
    "        print(f'{name} ({found_name})', end=': ')\n",
    "        print(f\"Found {n_tpf} TPFs\")\n",
    "        # Loop through the search result\n",
    "        for i in range(n_tpf):\n",
    "            print(f\"Downloading TPF {i+1}/{n_tpf}\", end='\\r')\n",
    "            # Get the exposure time\n",
    "            exptime = tpf[i].exptime\n",
    "            match = re.search(r'\\d+', str(exptime))\n",
    "            number = int(match.group())\n",
    "            # Create a folder for this exposure time within the star's folder\n",
    "            exptime_folder = f\"{star_folder}/exp_{number}\"\n",
    "            os.makedirs(exptime_folder, exist_ok=True)\n",
    "\n",
    "            try:\n",
    "                tpf_file = tpf[i].download()\n",
    "                fits_hdu = tpf_file.to_lightcurve(aperture_mask=tpf_file.pipeline_mask).to_fits()\n",
    "                header = fits_hdu[0].header\n",
    "                telescope, date, object = header['TELESCOP'], header['DATE'], header['OBJECT']\n",
    "                path = f\"{exptime_folder}/{name}_{telescope}_{date}_{object}_{i}.fits\"\n",
    "                fits_hdu.writeto(path, overwrite=True)\n",
    "                \n",
    "            except lk.LightkurveError as e:\n",
    "                print(f\"Error downloading TPF for {row['Star_Name']}: {e}\")\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"Error downloading TPF for {row['Star_Name']}: {e}\")\n",
    "            except IndexError as e:\n",
    "                print(f\"IndexError: {e}. Skipping iteration {i}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astroenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
